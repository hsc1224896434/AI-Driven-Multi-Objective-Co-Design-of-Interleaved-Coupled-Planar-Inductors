{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "import json"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-11T03:19:18.816616800Z",
     "start_time": "2023-09-11T03:19:17.982623900Z"
    }
   },
   "id": "261a7568e3c8564e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "移动文件的代码"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "34b91d0bef29e994"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 个文件已经从 E:\\ZR\\data_split\\train\\H 移动到 E:\\ZR\\data_split\\val\\H。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# 源文件夹路径\n",
    "source_folder = r'E:\\ZR\\data_split\\train\\H'\n",
    "# 目标文件夹路径\n",
    "target_folder = r'E:\\ZR\\data_split\\val\\H'\n",
    "\n",
    "# 获取源文件夹中所有文件的列表\n",
    "file_list = os.listdir(source_folder)\n",
    "\n",
    "# 计算每十个文件中需要移动的文件数量\n",
    "num_files_to_move = 2\n",
    "\n",
    "# 初始化一个列表来存储要移动的文件\n",
    "files_to_move = []\n",
    "\n",
    "# 遍历文件列表，每十个文件中移动最后的两个文件\n",
    "for i in range(0, len(file_list), 10):\n",
    "    # 获取最后的两个文件并添加到要移动的文件列表中\n",
    "    files_to_move.extend(file_list[i + 8:i + 10])\n",
    "\n",
    "# 移动文件到目标文件夹\n",
    "for file_name in files_to_move:\n",
    "    source_path = os.path.join(source_folder, file_name)\n",
    "    target_path = os.path.join(target_folder, file_name)\n",
    "    shutil.move(source_path, target_path)\n",
    "\n",
    "print(f'{num_files_to_move} 个文件已经从 {source_folder} 移动到 {target_folder}。')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T06:26:08.226603500Z",
     "start_time": "2023-09-08T06:25:34.173127400Z"
    }
   },
   "id": "4e16605b4e68226b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "将csv转成txt的代码"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c01895596a731005"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已经成功将每一行保存为TXT文件，文件名包含CSV文件名、材料名称和行号！\n"
     ]
    }
   ],
   "source": [
    "# 读取CSV文件\n",
    "M = '3C90'\n",
    "csv_file = f'./MagNet_data/{M}/merged_file.csv'\n",
    "output_folder = r'E:\\ZR\\data_split\\merged'  # 替换成你想要保存TXT文件的文件夹路径\n",
    "\n",
    "# 确保输出文件夹存在，如果不存在则创建它\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# 从文件路径中提取材料名称（假设材料名称位于路径的一部分）\n",
    "material_name = os.path.basename(os.path.dirname(csv_file))\n",
    "\n",
    "with open(csv_file, 'r', encoding='utf-8') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, )\n",
    "    # header = next(csvreader)  # 如果CSV文件有标题行，请跳过它\n",
    "\n",
    "    for line_number, row in enumerate(csvreader, start=1):\n",
    "        # 生成TXT文件名，格式为CSV文件名（不包含扩展名）+材料名称+行号\n",
    "        csv_filename = os.path.splitext(os.path.basename(csv_file))[0]\n",
    "        txt_filename = f\"{csv_filename}_{material_name}_{line_number}.txt\"\n",
    "        txt_filepath = os.path.join(output_folder, txt_filename)\n",
    "\n",
    "        # 将行内容保存到TXT文件中\n",
    "        with open(txt_filepath, 'w', encoding='utf-8') as txtfile:\n",
    "            txtfile.write(','.join(row))\n",
    "\n",
    "print(\"已经成功将每一行保存为TXT文件，文件名包含CSV文件名、材料名称和行号！\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-11T03:59:05.878111Z",
     "start_time": "2023-09-11T03:58:03.314928900Z"
    }
   },
   "id": "3403ef84bba28aca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "在把csv转成txt之前，先需要把几个csv文件合并成一个"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0f80ce1df735f34"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "M = 'N49'\n",
    "csv_file_0 = pd.read_csv(f'./MagNet_data/{M}/B_waveform[T].csv')\n",
    "csv_file_1 = pd.read_csv(f'./MagNet_data/{M}/Frequency[Hz].csv')\n",
    "csv_file_2 = pd.read_csv(f'./MagNet_data/{M}/Temperature[C].csv')\n",
    "csv_file_3 = pd.read_csv(f'./MagNet_data/{M}/Volumetric_losses[Wm-3].csv')\n",
    "result = pd.concat([csv_file_0, csv_file_1, csv_file_2, csv_file_3], axis=1)\n",
    "result.to_csv(f'./MagNet_data/{M}/merged_file.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-11T03:40:14.240141100Z",
     "start_time": "2023-09-11T03:39:50.886220Z"
    }
   },
   "id": "494b1d0b17fe9588"
  },
  {
   "cell_type": "markdown",
   "source": [
    "计算标准化参数"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf693d54bd71eeee"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "import torch\n",
    "Norm_Dic = {}\n",
    "\n",
    "M = 'N87'\n",
    "\n",
    "in_B = pd.read_csv(f'./MagNet_data/{M}/B_waveform[T].csv')\n",
    "in_B = torch.from_numpy(in_B.values).float().unsqueeze(2)\n",
    "\n",
    "in_T = pd.read_csv(f\"./MagNet_data/{M}/Temperature[C].csv\")\n",
    "in_T = torch.from_numpy(in_T.values).float().view(-1, 1)\n",
    "\n",
    "in_F = pd.read_csv(f\"./MagNet_data/{M}/Frequency[Hz].csv\")\n",
    "in_F = torch.from_numpy(in_F.values).float().view(-1, 1)\n",
    "\n",
    "gt_P = pd.read_csv(f\"./MagNet_data/{M}/Volumetric_losses[Wm-3].csv\")\n",
    "gt_P = torch.from_numpy(gt_P.values).float().view(-1, 1)\n",
    "\n",
    "# Transform\n",
    "in_F = torch.log(in_F)\n",
    "out_P = torch.log(gt_P)\n",
    "\n",
    "normB = [torch.mean(in_B), torch.std(in_B)]\n",
    "normF = [torch.mean(in_F), torch.std(in_F)]\n",
    "normT = [torch.mean(in_T), torch.std(in_T)]\n",
    "normP = [torch.mean(out_P), torch.std(out_P)]\n",
    "\n",
    "Norm_Dic[M] = [normB, normF, normT, normP]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-11T08:23:25.072338200Z",
     "start_time": "2023-09-11T08:23:14.030163100Z"
    }
   },
   "id": "c4905388caa6d0da"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "serializable_dict = {}\n",
    "for key, value in Norm_Dic.items():\n",
    "    serializable_value = [[item.numpy().tolist() for item in sublist] for sublist in value]\n",
    "    serializable_dict[key] = serializable_value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-11T08:23:30.744463200Z",
     "start_time": "2023-09-11T08:23:30.728834900Z"
    }
   },
   "id": "e42060241fb3a46e"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "with open('Norm_dic.json', 'w') as json_file:\n",
    "    json.dump(serializable_dict, json_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-11T08:23:46.463160600Z",
     "start_time": "2023-09-11T08:23:46.447526500Z"
    }
   },
   "id": "bbde6d90f12b2cb5"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type Tensor is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[50], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mjson\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNorm_dic.json\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m----> 3\u001B[0m     f\u001B[38;5;241m.\u001B[39mwrite(\u001B[43mjson\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdumps\u001B[49m\u001B[43m(\u001B[49m\u001B[43mNorm_Dic\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\torch201_1_zr\\lib\\json\\__init__.py:231\u001B[0m, in \u001B[0;36mdumps\u001B[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001B[0m\n\u001B[0;32m    226\u001B[0m \u001B[38;5;66;03m# cached encoder\u001B[39;00m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m skipkeys \u001B[38;5;129;01mand\u001B[39;00m ensure_ascii \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[0;32m    228\u001B[0m     check_circular \u001B[38;5;129;01mand\u001B[39;00m allow_nan \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[0;32m    229\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m indent \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m separators \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m\n\u001B[0;32m    230\u001B[0m     default \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m sort_keys \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kw):\n\u001B[1;32m--> 231\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_default_encoder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobj\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcls\u001B[39m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    233\u001B[0m     \u001B[38;5;28mcls\u001B[39m \u001B[38;5;241m=\u001B[39m JSONEncoder\n",
      "File \u001B[1;32m~\\.conda\\envs\\torch201_1_zr\\lib\\json\\encoder.py:199\u001B[0m, in \u001B[0;36mJSONEncoder.encode\u001B[1;34m(self, o)\u001B[0m\n\u001B[0;32m    195\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m encode_basestring(o)\n\u001B[0;32m    196\u001B[0m \u001B[38;5;66;03m# This doesn't pass the iterator directly to ''.join() because the\u001B[39;00m\n\u001B[0;32m    197\u001B[0m \u001B[38;5;66;03m# exceptions aren't as detailed.  The list call should be roughly\u001B[39;00m\n\u001B[0;32m    198\u001B[0m \u001B[38;5;66;03m# equivalent to the PySequence_Fast that ''.join() would do.\u001B[39;00m\n\u001B[1;32m--> 199\u001B[0m chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43miterencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m_one_shot\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    200\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(chunks, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m    201\u001B[0m     chunks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(chunks)\n",
      "File \u001B[1;32m~\\.conda\\envs\\torch201_1_zr\\lib\\json\\encoder.py:257\u001B[0m, in \u001B[0;36mJSONEncoder.iterencode\u001B[1;34m(self, o, _one_shot)\u001B[0m\n\u001B[0;32m    252\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    253\u001B[0m     _iterencode \u001B[38;5;241m=\u001B[39m _make_iterencode(\n\u001B[0;32m    254\u001B[0m         markers, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault, _encoder, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindent, floatstr,\n\u001B[0;32m    255\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkey_separator, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mitem_separator, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msort_keys,\n\u001B[0;32m    256\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mskipkeys, _one_shot)\n\u001B[1;32m--> 257\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_iterencode\u001B[49m\u001B[43m(\u001B[49m\u001B[43mo\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\torch201_1_zr\\lib\\json\\encoder.py:179\u001B[0m, in \u001B[0;36mJSONEncoder.default\u001B[1;34m(self, o)\u001B[0m\n\u001B[0;32m    160\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdefault\u001B[39m(\u001B[38;5;28mself\u001B[39m, o):\n\u001B[0;32m    161\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001B[39;00m\n\u001B[0;32m    162\u001B[0m \u001B[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001B[39;00m\n\u001B[0;32m    163\u001B[0m \u001B[38;5;124;03m    (to raise a ``TypeError``).\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    177\u001B[0m \n\u001B[0;32m    178\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 179\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mObject of type \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mo\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    180\u001B[0m                     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mis not JSON serializable\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: Object of type Tensor is not JSON serializable"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"Norm_dic.json\", \"w\") as f:\n",
    "    f.write(json.dumps(Norm_Dic))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-11T07:56:17.197552400Z",
     "start_time": "2023-09-11T07:56:17.032702800Z"
    }
   },
   "id": "da35ff580b9066b5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
